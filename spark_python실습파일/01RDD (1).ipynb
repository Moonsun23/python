{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fa840f-af50-4365-8a82-2b1903b16062",
   "metadata": {},
   "source": [
    "# spark 기본개념 잡기 : RDD\n",
    "+ Resillient Distributed Data\n",
    "+ 여러 분산노드에 걸쳐 저장하는 변경불가 데이터 집합을 의미\n",
    "+ RDD 생성은 직접 만들거나 파일을 통해 생성할 수 있음\n",
    "+ RDD는 transformation과 action으로 구성\n",
    "   - 기존 RDD의 데이터를 토대로 새로운 RDD를 만들어 냄\n",
    "   - RDD를 기반으로 무언가를 계산해서 결과를 만들어 냄\n",
    "+ RDD는 `Lazy 로딩 방식`을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba4efd-10ad-4a96-bad5-8f6d44911b0f",
   "metadata": {},
   "source": [
    "## 스파크 중요 개념 : RDD, dataframe\n",
    "* RDD : 탄력적이고 분산된 데이터셋\n",
    "* HDFS와는 달리 쓰기 불가능 데이터셋\n",
    "* 다양한 연산(map, reduce, count, filter, join) 수행 가능\n",
    "* 작업은 lazy하게 병렬로 수행되고 메모리에 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda2a8d-3fa6-40a3-b4df-6b5093723830",
   "metadata": {},
   "source": [
    "## History of Spark API\n",
    "* RDD   (2011)\n",
    "    + v1 부터 지원, 분산 데이터셋   \n",
    "    + 연산을 제어하는 코드 작성이 어려움\n",
    "* dataframe (2013) \n",
    "    + v1.3부터 지원\n",
    "    + 데이터를 스키마형태로 추상화, 속도 개선\n",
    "* dataset (2015) \n",
    "    + v1.6부터 지원\n",
    "    + 데이터의 자료형 검사, 직렬화 지원\n",
    "* dataset (2016) \n",
    "    + v2.0부터 지원\n",
    "    + dataframe과 dataset을 dataset으로 통합\n",
    "* 스파크 애플리케이션 개발 : RDD 이용, SparkContext 사용\n",
    "* SQL on Spark : dataset,dataframe 이용, SparkSession 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22faff56-ddeb-47af-bb06-820d4dfb7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# README.md 파일 읽어들임\n",
    "lines = sc.textFile('/usr/share/spark/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220632e5-0fe7-487b-905b-88237e71dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Structured Streaming for stream processing.',\n",
       " '',\n",
       " '<https://spark.apache.org/>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 읽어들인 라인들 중 10줄만 확인 : collect(action)\n",
    "lines.collect()[:10]\n",
    "# 아래생긴 빨간 줄이 실행했다는 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8904fb7-429f-4199-9ee2-7eb50d47a70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 읽어들인 라인수 확인 : count(action)\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f29d029-0913-45dc-803d-1e024a44d7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " '[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " '## Building Spark',\n",
       " 'Spark is built using [Apache Maven](https://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 읽어들인 라인들 중 Spark라는 단어를 찾음 : filter(transformation)\n",
    "filterLines = lines.filter(lambda x: \"Spark\" in x)\n",
    "filterLines.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981988d-a20c-48f1-9d6a-d17ed9dbfcb6",
   "metadata": {},
   "source": [
    "### Lazy 로딩방식 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8f9187-e81e-4287-bbaa-48789e7d9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘못된 위치의 파일을 읽으려고 시도\n",
    "lines = sc.textFile('usr/share/spark/README.md')\n",
    "\n",
    "# 잘못된 코드를 써도 오류출력이 안된다... 와이?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9eeea1-9745-4115-9844-eac14b479ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy loading 에 의해서 action이 호출되어야만 비로소 오류 출력!\n",
    "lines.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3c30e-3a53-4a6a-a96e-e2321cb3cc2a",
   "metadata": {},
   "source": [
    "## RDD 생성\n",
    "+ 직접 생성한 데이터로 만들거나\n",
    "   - sc.parallelize(리스트)\n",
    "+ 외부 데이터로 만드는 방법 존재\n",
    "   - sc.textFile(경로/파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d9f67a-f1aa-4b4b-99b2-6c9f7d31f4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Hello, World^^','Hello, Python>.<','Hello, RDD~!']\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d866481-0aab-4673-98b1-16e1a14083ad",
   "metadata": {},
   "source": [
    "### 직책별 사원수 조회하는 RDD 코드 작성\n",
    "+ employees.csv 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78e658e7-3e96-45cb-9f9a-63ec1c623de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMPLOYEE_ID,FIRST_NAME,LAST_NAME,EMAIL,PHONE_NUMBER,HIRE_DATE,JOB_ID,SALARY,COMMISSION_PCT,MANAGER_ID,DEPARTMENT_ID',\n",
       " '100,Steven,King,SKING,515.123.4567,2003-06-17,AD_PRES,24000.00,,,90',\n",
       " '101,Neena,Kochhar,NKOCHHAR,515.123.4568,2005-09-21,AD_VP,17000.00,,100,90',\n",
       " '102,Lex,De Haan,LDEHAAN,515.123.4569,2001-01-13,AD_VP,17000.00,,100,90',\n",
       " '103,Alexander,Hunold,AHUNOLD,590.423.4567,2006-01-03,IT_PROG,9000.00,,102,60']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = sc.textFile('data/employees.csv')\n",
    "emp.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eda610c-d062-4089-bbba-ec10bd969c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EMPLOYEE_ID,FIRST_NAME,LAST_NAME,EMAIL,PHONE_NUMBER,HIRE_DATE,JOB_ID,SALARY,COMMISSION_PCT,MANAGER_ID,DEPARTMENT_ID'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 헤더 제외하고 데이터만 골라냄\n",
    "header = emp.first()\n",
    "header # 여기는 헤더를 뽑아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ced04b-9b26-427b-8a11-da4f970f5e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100,Steven,King,SKING,515.123.4567,2003-06-17,AD_PRES,24000.00,,,90'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = emp.filter(lambda x: header != x)\n",
    "emp.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7654ad4-972a-4942-ae1e-ed8d27d90de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 'Steven'),\n",
       " ('AD_VP', 'Neena'),\n",
       " ('AD_VP', 'Lex'),\n",
       " ('IT_PROG', 'Alexander'),\n",
       " ('IT_PROG', 'Bruce')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사원데이터에서 ,로 각 컬럼을 분리하고\n",
    "# 이름과 직책을 추출함\n",
    "emp2=emp.map(lambda x: (x.split(',')[6], x.split(',')[1]))\n",
    "emp2.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7f17f7-c563-424f-9d8c-4d201fc81d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 1), ('AD_VP', 1), ('AD_VP', 1), ('IT_PROG', 1), ('IT_PROG', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추출한 데이터에서 직책을 1로 매핑\n",
    "maps = emp2.mapValues(lambda x: 1)\n",
    "maps.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "346ad557-4189-4597-91c2-6ddf2cfcc1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 1),\n",
       " ('AD_VP', 2),\n",
       " ('IT_PROG', 5),\n",
       " ('FI_MGR', 1),\n",
       " ('FI_ACCOUNT', 5)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 같은 직책끼리 모아서 집계처리함\n",
    "reduces = maps.reduceByKey(lambda x, y: x + y)\n",
    "reduces.collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea108ccd-1ad3-46d7-b698-ace738bad08a",
   "metadata": {},
   "source": [
    "### 타이타닉 승객의 생존자/사망자 수를 조회하는 RDD코드를 작성하세요\n",
    "+ titanic.csv 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b4ec050-416d-4eca-a9af-eac367bcffc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pclass,survived,name,sex,age,sibsp,parch,ticket,fare,cabin,embarked',\n",
       " '1,1,\"Allen, Miss. Elisabeth Walton\",female,29,0,0,24160,211.3375,B5,S',\n",
       " '1,1,\"Allison, Master. Hudson Trevor\",male,0.9167,1,2,113781,151.5500,C22 C26,S',\n",
       " '1,0,\"Allison, Miss. Helen Loraine\",female,2,1,2,113781,151.5500,C22 C26,S',\n",
       " '1,0,\"Allison, Mr. Hudson Joshua Creighton\",male,30,1,2,113781,151.5500,C22 C26,S']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tita = sc.textFile('data/titanic.csv')\n",
    "tita.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c9ee72f-b988-4a5e-8e9c-13a7c0b0f216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,1,\"Allen, Miss. Elisabeth Walton\",female,29,0,0,24160,211.3375,B5,S'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = tita.first()\n",
    "tita = tita.filter(lambda x: header != x)\n",
    "tita.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d07c339-10cc-4998-a65e-5d9877da6fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'female'),\n",
       " ('1', 'male'),\n",
       " ('0', 'female'),\n",
       " ('0', 'male'),\n",
       " ('0', 'female')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tita2=tita.map(lambda x: (x.split(',')[1], x.split(',')[4]))\n",
    "tita2.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf5f81d1-8bce-4b19-a8fc-a82af1167457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1), ('1', 1), ('0', 1), ('0', 1), ('0', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps = tita2.mapValues(lambda x:1)\n",
    "maps.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f44af719-6f1a-4e5b-a769-2d2e968d1724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 500), ('0', 809), ('', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduces = maps.reduceByKey(lambda x, y: x + y)\n",
    "reduces.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0a022-60fc-40b3-984f-dc021e44e268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f4529-5ca6-468a-bc2b-28d8cc697764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
